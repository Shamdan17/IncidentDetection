{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dataset\n",
    "from util import (\n",
    "    get_place_to_index_mapping,\n",
    "    get_incident_to_index_mapping\n",
    ")\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.layers import Dense, Flatten, Permute\n",
    "from keras import Sequential\n",
    "import keras.backend as kb\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_path = \"/kuacc/users/asafaya19/cv-project\"\n",
    "train_json = os.path.join(abs_path ,\"eccv_train.json\")\n",
    "val_json = os.path.join(abs_path ,\"eccv_val.json\")\n",
    "data_dir = os.path.join(abs_path, \"data\")\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "val_dir = os.path.join(data_dir, \"val\")\n",
    "\n",
    "train_paths = json.loads(open(train_json).readline())\n",
    "val_paths = json.loads(open(val_json).readline())\n",
    "\n",
    "place_to_idx = get_place_to_index_mapping()\n",
    "incident_to_idx = get_incident_to_index_mapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(paths,file_dir, threshold=1000):\n",
    "    train_set = []\n",
    "    for path in tqdm(paths, leave=False):\n",
    "    #     print(path)\n",
    "        if not os.path.exists(os.path.join(file_dir, path)):\n",
    "            continue\n",
    "        # Make sure image is not corrupt, try importing it\n",
    "        try:\n",
    "            img = PIL.Image.open(os.path.join(file_dir, path))\n",
    "            img.resize((224, 224))\n",
    "        except:\n",
    "            continue\n",
    "        nump = len(place_to_idx) + 1\n",
    "        numi = len(incident_to_idx) + 1\n",
    "        place_labels = np.zeros(nump, np.float32)\n",
    "        place_weights = np.zeros(nump, np.float32)\n",
    "        incident_labels = np.zeros(numi, np.float32)\n",
    "        incident_weights = np.zeros(numi, np.float32)\n",
    "\n",
    "        incidents = paths[path][\"incidents\"]\n",
    "        for k in incidents:\n",
    "            lbl = incidents[k]\n",
    "            if lbl==1:\n",
    "                # We are sure this instance is only this incident\n",
    "                incident_labels[incident_to_idx[k]]=1\n",
    "                incident_weights = np.ones(numi, np.float32)\n",
    "            else:\n",
    "                # We are only sure that this image is not that incident\n",
    "                incident_weights[incident_to_idx[k]]=1\n",
    "        if len(incidents)==0:\n",
    "            # No incident\n",
    "            incident_labels[-1]=1\n",
    "            incident_weights = np.ones(numi, np.float32)\n",
    "\n",
    "        places = paths[path][\"places\"]\n",
    "        for k in places:\n",
    "            lbl = places[k]\n",
    "            if lbl==1:\n",
    "                # We are sure this instance is only this incident\n",
    "                place_labels[place_to_idx[k]]=1\n",
    "                place_weights = np.ones(nump, np.float32)\n",
    "            else:\n",
    "                # We are only sure that this image is not that incident\n",
    "                place_weights[place_to_idx[k]]=1\n",
    "        if len(places)==0:\n",
    "            # No place\n",
    "            place_labels[-1]=1\n",
    "            place_weights = np.ones(nump, np.float32)\n",
    "\n",
    "\n",
    "        train_set.append({\n",
    "            \"path\":path,\n",
    "            \"incident_labels\":incident_labels,\n",
    "            \"incident_weights\":incident_weights,\n",
    "            \"incidents\":np.vstack((incident_labels, incident_weights)),\n",
    "            \"place_labels\":place_labels,\n",
    "            \"place_weights\":place_weights,\n",
    "            \"place\":np.vstack((place_labels, place_weights))\n",
    "        })\n",
    "        if len(train_set)>=threshold:\n",
    "            break\n",
    "    return train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getpreprocessfunc():\n",
    "    mean = np.asarray([0.485, 0.456, 0.406]).reshape(3, 1, 1).astype(np.float32)\n",
    "    std = np.asarray([0.229, 0.224, 0.225]).reshape(3, 1, 1).astype(np.float32)\n",
    "    def preprocessfunc(img):\n",
    "#         print(img.shape)\n",
    "#         print(type(img))\n",
    "        img /= 255\n",
    "        img -= mean\n",
    "        img /= std\n",
    "        return img\n",
    "    return preprocessfunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enclosure to retain state\n",
    "def get_weighted_accuracy():\n",
    "    m = keras.metrics.CategoricalAccuracy()\n",
    "    def weighted_accuracy(y_true, y_preds):\n",
    "        y_true = tf.reshape(y_true, (bs, 2, -1))\n",
    "        y_true_lbls = y_true[:,0,:]\n",
    "        return m(y_true_lbls, y_preds)\n",
    "    return weighted_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_loss(y_true, y_preds):\n",
    "    bce = keras.losses.BinaryCrossentropy(keras.losses.Reduction.NONE)\n",
    "#     tf.print(y_true)\n",
    "#     tf.print(y_preds)\n",
    "#     print(y_true.shape)\n",
    "#     print(y_preds.shape)\n",
    "    bs = y_true.shape[0]\n",
    "    \n",
    "    print(y_true.shape, y_preds.shape)\n",
    "    \n",
    "    y_true = tf.reshape(y_true, (bs, 2, -1))\n",
    "    y_true_lbls = y_true[:,0,:]\n",
    "    y_true_weights = y_true[:,1,:]\n",
    "    bce_loss = bce(y_true_lbls, y_preds)\n",
    "#     import pdb\n",
    "#     pdb.set_trace()\n",
    "#     return bce_loss\n",
    "    return tf.reduce_sum(tf.multiply(bce_loss, y_true_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "\n",
    "class FinalModel(keras.Model):\n",
    "    def __init__(self, trunk_model, incident_weights, place_weights):\n",
    "        super(FinalModel, self).__init__()\n",
    "        self.permute = Permute((2, 3, 1))\n",
    "        self.cropped = keras.layers.experimental.preprocessing.RandomCrop(224, 224)\n",
    "        self.permuteback = Permute((3, 1, 2))\n",
    "        self.trunk_model = trunk_model\n",
    "        self.incident_proj = Dense(len(incident_to_idx), input_shape=(1024,), name=\"incidents_projection\", weights=incident_weights)\n",
    "        self.places_proj = Dense(len(place_to_idx), input_shape=(1024,), name=\"places_projection\", weights=place_weights)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.permute(inputs)\n",
    "        x = self.cropped(x)\n",
    "        x = self.permuteback(x)\n",
    "        x = self.trunk_model(x)\n",
    "        \n",
    "        return self.incident_proj(x), self.places_proj(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    }
   ],
   "source": [
    "train_set = get_dataset(train_paths, train_dir, 1000)\n",
    "val_set = get_dataset(val_paths, val_dir)\n",
    "\n",
    "train_df = pd.DataFrame(train_set)\n",
    "val_df = pd.DataFrame(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "imgen = ImageDataGenerator(\n",
    "    horizontal_flip=True,\n",
    "    preprocessing_function=getpreprocessfunc(),\n",
    ")\n",
    "\n",
    "imgen = imgen.flow_from_dataframe(\n",
    "    train_df,\n",
    "    directory=train_dir,\n",
    "    x_col=\"path\",\n",
    "    y_col=[\"incidents\", \"place\"],\n",
    "    weight_col=None,\n",
    "    target_size=(256, 256),\n",
    "    color_mode=\"rgb\",\n",
    "    classes=None,\n",
    "    class_mode=\"multi_output\",\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    seed=True,\n",
    "    save_to_dir=None,\n",
    "    save_prefix=\"\",\n",
    "    save_format=\"png\",\n",
    "    subset=None,\n",
    "    interpolation=\"nearest\",\n",
    "    validate_filenames=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {\n",
    "    \"output_1\": weighted_loss,\n",
    "    \"output_2\": weighted_loss,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adam(lr=1e-5)\n",
    "\n",
    "trunk_model = model_from_json(open(os.path.join(abs_path,'models/trunk.json'), 'r').read())\n",
    "trunk_model.load_weights(os.path.join(abs_path,\"models/trunk.h5\"))\n",
    "place_w = np.load(os.path.join(abs_path,'models/place_w.npy')).T\n",
    "place_b = np.load(os.path.join(abs_path,'models/place_b.npy')).T\n",
    "incident_w = np.load(os.path.join(abs_path,'models/incident_w.npy')).T\n",
    "incident_b = np.load(os.path.join(abs_path,'models/incident_b.npy')).T\n",
    "\n",
    "mdl = FinalModel(trunk_model, [incident_w, incident_b], [place_w, place_b])\n",
    "\n",
    "mdl.compile(optimizer=opt, loss=losses, metrics=[get_weighted_accuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(imgen)\n",
    "o = mdl(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import sklearn\n",
    "from sklearn.metrics._base import type_of_target\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "def fixed_precision_recall_curve(y_true, probas_pred, *, pos_label=None,\n",
    "                           sample_weight=None):\n",
    "\n",
    "    fps, tps, thresholds = sklearn.metrics._ranking._binary_clf_curve(y_true, probas_pred,\n",
    "                                             pos_label=pos_label,\n",
    "                                             sample_weight=sample_weight)\n",
    "\n",
    "    precision = tps / (tps + fps)\n",
    "    precision[np.isnan(precision)] = 0\n",
    "    recall = np.ones(tps.size) if tps[-1] == 0 else tps / tps[-1]\n",
    "\n",
    "    # stop when full recall attained\n",
    "    # and reverse the outputs so recall is decreasing\n",
    "    last_ind = tps.searchsorted(tps[-1])\n",
    "    sl = slice(last_ind, None, -1)\n",
    "    return np.r_[precision[sl], 1], np.r_[recall[sl], 0], thresholds[sl]\n",
    "\n",
    "\n",
    "def average_precision_score(y_true, y_score, *, average=\"macro\", pos_label=1,\n",
    "                            sample_weight=None):\n",
    "    def _binary_uninterpolated_average_precision(\n",
    "            y_true, y_score, pos_label=1, sample_weight=None):\n",
    "        precision, recall, _ = fixed_precision_recall_curve(\n",
    "            y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)\n",
    "        # Return the step function integral\n",
    "        # The following works because the last entry of precision is\n",
    "        # guaranteed to be 1, as returned by precision_recall_curve\n",
    "        return -np.sum(np.diff(recall) * np.array(precision)[:-1])\n",
    "\n",
    "    y_type = type_of_target(y_true)\n",
    "    if y_type == \"multilabel-indicator\" and pos_label != 1:\n",
    "        raise ValueError(\"Parameter pos_label is fixed to 1 for \"\n",
    "                         \"multilabel-indicator y_true. Do not set \"\n",
    "                         \"pos_label or set pos_label to 1.\")\n",
    "    elif y_type == \"binary\":\n",
    "        # Convert to Python primitive type to avoid NumPy type / Python str\n",
    "        # comparison. See https://github.com/numpy/numpy/issues/6784\n",
    "        present_labels = np.unique(y_true).tolist()\n",
    "        if len(present_labels) == 2 and pos_label not in present_labels:\n",
    "            raise ValueError(\n",
    "                f\"pos_label={pos_label} is not a valid label. It should be \"\n",
    "                f\"one of {present_labels}\"\n",
    "            )\n",
    "    average_precision = partial(_binary_uninterpolated_average_precision,\n",
    "                                pos_label=pos_label)\n",
    "    return sklearn.metrics._base._average_binary_score(average_precision, y_true, y_score,\n",
    "                                 average, sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1856/57207 [00:37<23:43, 38.89it/s]/kuacc/users/asafaya19/anaconda3/envs/ml-graphs/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:792: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      " 12%|█▏        | 6622/57207 [02:21<13:09, 64.05it/s]  /kuacc/users/asafaya19/anaconda3/envs/ml-graphs/lib/python3.7/site-packages/PIL/Image.py:2837: DecompressionBombWarning: Image size (134599200 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  DecompressionBombWarning,\n",
      " 41%|████▏     | 23642/57207 [08:55<09:07, 61.26it/s]  /kuacc/users/asafaya19/anaconda3/envs/ml-graphs/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:771: UserWarning: Possibly corrupt EXIF data.  Expecting to read 9 bytes but only got 8. Skipping tag 33432\n",
      "  \"Possibly corrupt EXIF data.  \"\n",
      " 57%|█████▋    | 32355/57207 [12:13<10:01, 41.30it/s]/kuacc/users/asafaya19/anaconda3/envs/ml-graphs/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:771: UserWarning: Possibly corrupt EXIF data.  Expecting to read 36 bytes but only got 35. Skipping tag 33432\n",
      "  \"Possibly corrupt EXIF data.  \"\n",
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 47649 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "val_set = get_dataset(val_paths, val_dir, threshold=100000)\n",
    "val_df = pd.DataFrame(val_set)\n",
    "\n",
    "imgen = ImageDataGenerator(\n",
    "    preprocessing_function=getpreprocessfunc(),\n",
    ")\n",
    "\n",
    "imgen = imgen.flow_from_dataframe(\n",
    "    val_df,\n",
    "    directory=val_dir,\n",
    "    x_col=\"path\",\n",
    "    y_col=[\"incidents\", \"place\"],\n",
    "    weight_col=None,\n",
    "    target_size=(256, 256),\n",
    "    color_mode=\"rgb\",\n",
    "    classes=None,\n",
    "    class_mode=\"multi_output\",\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    seed=True,\n",
    "    save_to_dir=None,\n",
    "    save_prefix=\"\",\n",
    "    save_format=\"png\",\n",
    "    subset=None,\n",
    "    interpolation=\"nearest\",\n",
    "    validate_filenames=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:31<00:00,  1.97s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.1065806187405204, 0.3706045208781187)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgen.reset()\n",
    "\n",
    "nbatches = 0\n",
    "imap = 0\n",
    "pmap = 0\n",
    "\n",
    "for x, y in tqdm(imgen):\n",
    "    \n",
    "    y1 = y[0]\n",
    "    y2 = y[1]\n",
    "\n",
    "    out = mdl.predict(x)\n",
    "    \n",
    "    iout = sigmoid(out[0])\n",
    "    pout = sigmoid(out[1])\n",
    "    \n",
    "    itrue = y1[:,0,:-1]\n",
    "    ptrue = y2[:,0,:-1]\n",
    "    \n",
    "    imap += average_precision_score(itrue, iout)\n",
    "    pmap += average_precision_score(ptrue, pout)\n",
    "    nbatches += 1\n",
    "    \n",
    "    if nbatches > np.ceil(val_df.shape[0] / 64):\n",
    "        break\n",
    "    \n",
    "pmap.item() / nbatches, imap.item() / nbatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
